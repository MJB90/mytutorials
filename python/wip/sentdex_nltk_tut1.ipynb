{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "writing= \"Modern NLP algorithms are based on machine learning, especially statistical machine learning. The paradigm of machine learning is different from that of most prior attempts at language processing. Prior implementations of language-processing tasks typically involved the direct hand coding of large sets of rules. The machine-learning paradigm calls instead for using general learning algorithms - often, although not always, grounded in statistical inference - to automatically learn such rules through the analysis of large corpora of typical real-world examples. A corpus (plural, 'corpora') is a set of documents (or sometimes, individual sentences) that have been hand-annotated with the correct values to be learned.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modern NLP algorithms are based on machine learning, especially statistical machine learning.',\n",
       " 'The paradigm of machine learning is different from that of most prior attempts at language processing.',\n",
       " 'Prior implementations of language-processing tasks typically involved the direct hand coding of large sets of rules.',\n",
       " 'The machine-learning paradigm calls instead for using general learning algorithms - often, although not always, grounded in statistical inference - to automatically learn such rules through the analysis of large corpora of typical real-world examples.',\n",
       " \"A corpus (plural, 'corpora') is a set of documents (or sometimes, individual sentences) that have been hand-annotated with the correct values to be learned.\"]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modern',\n",
       " 'NLP',\n",
       " 'algorithms',\n",
       " 'are',\n",
       " 'based',\n",
       " 'on',\n",
       " 'machine',\n",
       " 'learning',\n",
       " ',',\n",
       " 'especially',\n",
       " 'statistical',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'The',\n",
       " 'paradigm',\n",
       " 'of',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'is',\n",
       " 'different',\n",
       " 'from',\n",
       " 'that',\n",
       " 'of',\n",
       " 'most',\n",
       " 'prior',\n",
       " 'attempts',\n",
       " 'at',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'Prior',\n",
       " 'implementations',\n",
       " 'of',\n",
       " 'language-processing',\n",
       " 'tasks',\n",
       " 'typically',\n",
       " 'involved',\n",
       " 'the',\n",
       " 'direct',\n",
       " 'hand',\n",
       " 'coding',\n",
       " 'of',\n",
       " 'large',\n",
       " 'sets',\n",
       " 'of',\n",
       " 'rules',\n",
       " '.',\n",
       " 'The',\n",
       " 'machine-learning',\n",
       " 'paradigm',\n",
       " 'calls',\n",
       " 'instead',\n",
       " 'for',\n",
       " 'using',\n",
       " 'general',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " '-',\n",
       " 'often',\n",
       " ',',\n",
       " 'although',\n",
       " 'not',\n",
       " 'always',\n",
       " ',',\n",
       " 'grounded',\n",
       " 'in',\n",
       " 'statistical',\n",
       " 'inference',\n",
       " '-',\n",
       " 'to',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'such',\n",
       " 'rules',\n",
       " 'through',\n",
       " 'the',\n",
       " 'analysis',\n",
       " 'of',\n",
       " 'large',\n",
       " 'corpora',\n",
       " 'of',\n",
       " 'typical',\n",
       " 'real-world',\n",
       " 'examples',\n",
       " '.',\n",
       " 'A',\n",
       " 'corpus',\n",
       " '(',\n",
       " 'plural',\n",
       " ',',\n",
       " \"'corpora\",\n",
       " \"'\",\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'set',\n",
       " 'of',\n",
       " 'documents',\n",
       " '(',\n",
       " 'or',\n",
       " 'sometimes',\n",
       " ',',\n",
       " 'individual',\n",
       " 'sentences',\n",
       " ')',\n",
       " 'that',\n",
       " 'have',\n",
       " 'been',\n",
       " 'hand-annotated',\n",
       " 'with',\n",
       " 'the',\n",
       " 'correct',\n",
       " 'values',\n",
       " 'to',\n",
       " 'be',\n",
       " 'learned',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words= set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'a',\n",
       " u'about',\n",
       " u'above',\n",
       " u'after',\n",
       " u'again',\n",
       " u'against',\n",
       " u'ain',\n",
       " u'all',\n",
       " u'am',\n",
       " u'an',\n",
       " u'and',\n",
       " u'any',\n",
       " u'are',\n",
       " u'aren',\n",
       " u'as',\n",
       " u'at',\n",
       " u'be',\n",
       " u'because',\n",
       " u'been',\n",
       " u'before',\n",
       " u'being',\n",
       " u'below',\n",
       " u'between',\n",
       " u'both',\n",
       " u'but',\n",
       " u'by',\n",
       " u'can',\n",
       " u'couldn',\n",
       " u'd',\n",
       " u'did',\n",
       " u'didn',\n",
       " u'do',\n",
       " u'does',\n",
       " u'doesn',\n",
       " u'doing',\n",
       " u'don',\n",
       " u'down',\n",
       " u'during',\n",
       " u'each',\n",
       " u'few',\n",
       " u'for',\n",
       " u'from',\n",
       " u'further',\n",
       " u'had',\n",
       " u'hadn',\n",
       " u'has',\n",
       " u'hasn',\n",
       " u'have',\n",
       " u'haven',\n",
       " u'having',\n",
       " u'he',\n",
       " u'her',\n",
       " u'here',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'him',\n",
       " u'himself',\n",
       " u'his',\n",
       " u'how',\n",
       " u'i',\n",
       " u'if',\n",
       " u'in',\n",
       " u'into',\n",
       " u'is',\n",
       " u'isn',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'just',\n",
       " u'll',\n",
       " u'm',\n",
       " u'ma',\n",
       " u'me',\n",
       " u'mightn',\n",
       " u'more',\n",
       " u'most',\n",
       " u'mustn',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'needn',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'now',\n",
       " u'o',\n",
       " u'of',\n",
       " u'off',\n",
       " u'on',\n",
       " u'once',\n",
       " u'only',\n",
       " u'or',\n",
       " u'other',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'out',\n",
       " u'over',\n",
       " u'own',\n",
       " u're',\n",
       " u's',\n",
       " u'same',\n",
       " u'shan',\n",
       " u'she',\n",
       " u'should',\n",
       " u'shouldn',\n",
       " u'so',\n",
       " u'some',\n",
       " u'such',\n",
       " u't',\n",
       " u'than',\n",
       " u'that',\n",
       " u'the',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'them',\n",
       " u'themselves',\n",
       " u'then',\n",
       " u'there',\n",
       " u'these',\n",
       " u'they',\n",
       " u'this',\n",
       " u'those',\n",
       " u'through',\n",
       " u'to',\n",
       " u'too',\n",
       " u'under',\n",
       " u'until',\n",
       " u'up',\n",
       " u've',\n",
       " u'very',\n",
       " u'was',\n",
       " u'wasn',\n",
       " u'we',\n",
       " u'were',\n",
       " u'weren',\n",
       " u'what',\n",
       " u'when',\n",
       " u'where',\n",
       " u'which',\n",
       " u'while',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'why',\n",
       " u'will',\n",
       " u'with',\n",
       " u'won',\n",
       " u'wouldn',\n",
       " u'y',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words= word_tokenize(writing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modern',\n",
       " 'NLP',\n",
       " 'algorithms',\n",
       " 'based',\n",
       " 'machine',\n",
       " 'learning',\n",
       " ',',\n",
       " 'especially',\n",
       " 'statistical',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'The',\n",
       " 'paradigm',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'different',\n",
       " 'prior',\n",
       " 'attempts',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'Prior',\n",
       " 'implementations',\n",
       " 'language-processing',\n",
       " 'tasks',\n",
       " 'typically',\n",
       " 'involved',\n",
       " 'direct',\n",
       " 'hand',\n",
       " 'coding',\n",
       " 'large',\n",
       " 'sets',\n",
       " 'rules',\n",
       " '.',\n",
       " 'The',\n",
       " 'machine-learning',\n",
       " 'paradigm',\n",
       " 'calls',\n",
       " 'instead',\n",
       " 'using',\n",
       " 'general',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " '-',\n",
       " 'often',\n",
       " ',',\n",
       " 'although',\n",
       " 'always',\n",
       " ',',\n",
       " 'grounded',\n",
       " 'statistical',\n",
       " 'inference',\n",
       " '-',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'rules',\n",
       " 'analysis',\n",
       " 'large',\n",
       " 'corpora',\n",
       " 'typical',\n",
       " 'real-world',\n",
       " 'examples',\n",
       " '.',\n",
       " 'A',\n",
       " 'corpus',\n",
       " '(',\n",
       " 'plural',\n",
       " ',',\n",
       " \"'corpora\",\n",
       " \"'\",\n",
       " ')',\n",
       " 'set',\n",
       " 'documents',\n",
       " '(',\n",
       " 'sometimes',\n",
       " ',',\n",
       " 'individual',\n",
       " 'sentences',\n",
       " ')',\n",
       " 'hand-annotated',\n",
       " 'correct',\n",
       " 'values',\n",
       " 'learned',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filtered_sentence = [w for w in words if w not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Modern',\n",
       " 'NLP',\n",
       " 'algorithms',\n",
       " 'based',\n",
       " 'machine',\n",
       " 'learning',\n",
       " ',',\n",
       " 'especially',\n",
       " 'statistical',\n",
       " 'machine',\n",
       " 'learning',\n",
       " '.',\n",
       " 'The',\n",
       " 'paradigm',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'different',\n",
       " 'prior',\n",
       " 'attempts',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'Prior',\n",
       " 'implementations',\n",
       " 'language-processing',\n",
       " 'tasks',\n",
       " 'typically',\n",
       " 'involved',\n",
       " 'direct',\n",
       " 'hand',\n",
       " 'coding',\n",
       " 'large',\n",
       " 'sets',\n",
       " 'rules',\n",
       " '.',\n",
       " 'The',\n",
       " 'machine-learning',\n",
       " 'paradigm',\n",
       " 'calls',\n",
       " 'instead',\n",
       " 'using',\n",
       " 'general',\n",
       " 'learning',\n",
       " 'algorithms',\n",
       " '-',\n",
       " 'often',\n",
       " ',',\n",
       " 'although',\n",
       " 'always',\n",
       " ',',\n",
       " 'grounded',\n",
       " 'statistical',\n",
       " 'inference',\n",
       " '-',\n",
       " 'automatically',\n",
       " 'learn',\n",
       " 'rules',\n",
       " 'analysis',\n",
       " 'large',\n",
       " 'corpora',\n",
       " 'typical',\n",
       " 'real-world',\n",
       " 'examples',\n",
       " '.',\n",
       " 'A',\n",
       " 'corpus',\n",
       " '(',\n",
       " 'plural',\n",
       " ',',\n",
       " \"'corpora\",\n",
       " \"'\",\n",
       " ')',\n",
       " 'set',\n",
       " 'documents',\n",
       " '(',\n",
       " 'sometimes',\n",
       " ',',\n",
       " 'individual',\n",
       " 'sentences',\n",
       " ')',\n",
       " 'hand-annotated',\n",
       " 'correct',\n",
       " 'values',\n",
       " 'learned',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_words= [\"python\", \"pythoner\", \"pythoning\", \"pythonic\", \"pythoned\", \"pythonly\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "python\n",
      "pythonli\n"
     ]
    }
   ],
   "source": [
    "for w in example_words:\n",
    "    print ps.stem(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "example_words= ['caresses', 'flies', 'dies', 'mules', 'denied',\n",
    "...            'died', 'agreed', 'owned', 'humbled', 'sized',\n",
    "...            'meeting', 'stating', 'siezing', 'itemization',\n",
    "...            'sensational', 'traditional', 'reference', 'colonizer',\n",
    "...            'plotted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'caress',\n",
       " u'fli',\n",
       " u'die',\n",
       " u'mule',\n",
       " u'deni',\n",
       " u'die',\n",
       " u'agre',\n",
       " u'own',\n",
       " u'humbl',\n",
       " u'size',\n",
       " u'meet',\n",
       " u'state',\n",
       " u'siez',\n",
       " u'item',\n",
       " u'sensat',\n",
       " u'tradit',\n",
       " u'refer',\n",
       " u'colon',\n",
       " u'plot']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ps.stem(w) for w in example_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fun = \"It is very important to be pythonic when pythoning with python. All pythoners have pythoned wrong at least once in their life.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words= word_tokenize(fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'It',\n",
       " u'is',\n",
       " u'veri',\n",
       " u'import',\n",
       " u'to',\n",
       " u'be',\n",
       " u'python',\n",
       " u'when',\n",
       " u'python',\n",
       " u'with',\n",
       " u'python',\n",
       " u'.',\n",
       " u'All',\n",
       " u'python',\n",
       " u'have',\n",
       " u'python',\n",
       " u'wrong',\n",
       " u'at',\n",
       " u'least',\n",
       " u'onc',\n",
       " u'in',\n",
       " u'their',\n",
       " u'life',\n",
       " u'.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ps.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import state_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "test_text = state_union.raw(\"2006-GWBush.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenized = custom_sent_tokenizer.tokenize(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "341"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u\"PRESIDENT GEORGE W. BUSH'S ADDRESS BEFORE A JOINT SESSION OF THE CONGRESS ON THE STATE OF THE UNION\\n \\nJanuary 31, 2006\\n\\nTHE PRESIDENT: Thank you all.\",\n",
       " u'Mr. Speaker, Vice President Cheney, members of Congress, members of the Supreme Court and diplomatic corps, distinguished guests, and fellow citizens: Today our nation lost a beloved, graceful, courageous woman who called America to its founding ideals and carried on a noble dream.',\n",
       " u'Tonight we are comforted by the hope of a glad reunion with the husband who was taken so long ago, and we are grateful for the good life of Coretta Scott King.',\n",
       " u'(Applause.)',\n",
       " u'President George W. Bush reacts to applause during his State of the Union Address at the Capitol, Tuesday, Jan. 31, 2006.',\n",
       " u\"White House photo by Eric DraperEvery time I'm invited to this rostrum, I'm humbled by the privilege, and mindful of the history we've seen together.\",\n",
       " u'We have gathered under this Capitol dome in moments of national mourning and national achievement.',\n",
       " u'We have served America through one of the most consequential periods of our history -- and it has been my honor to serve with you.',\n",
       " u'In a system of two parties, two chambers, and two elected branches, there will always be differences and debate.',\n",
       " u'But even tough debates can be conducted in a civil tone, and our differences cannot be allowed to harden into anger.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt= tokenized[:10]\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def processContent():\n",
    "    try:\n",
    "        for i in tt:\n",
    "            words = word_tokenize(i)\n",
    "            tagged= nltk.pos_tag(words)\n",
    "            print tagged\n",
    "            \n",
    "    except Exception as e:\n",
    "        print str(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'PRESIDENT', 'NNP'), (u'GEORGE', 'NNP'), (u'W.', 'NNP'), (u'BUSH', 'NNP'), (u\"'S\", 'POS'), (u'ADDRESS', 'NNP'), (u'BEFORE', 'IN'), (u'A', 'NNP'), (u'JOINT', 'NNP'), (u'SESSION', 'NNP'), (u'OF', 'IN'), (u'THE', 'NNP'), (u'CONGRESS', 'NNP'), (u'ON', 'NNP'), (u'THE', 'NNP'), (u'STATE', 'NNP'), (u'OF', 'IN'), (u'THE', 'NNP'), (u'UNION', 'NNP'), (u'January', 'NNP'), (u'31', 'CD'), (u',', ','), (u'2006', 'CD'), (u'THE', 'NNP'), (u'PRESIDENT', 'NNP'), (u':', ':'), (u'Thank', 'NNP'), (u'you', 'PRP'), (u'all', 'DT'), (u'.', '.')]\n",
      "[(u'Mr.', 'NNP'), (u'Speaker', 'NNP'), (u',', ','), (u'Vice', 'NNP'), (u'President', 'NNP'), (u'Cheney', 'NNP'), (u',', ','), (u'members', 'NNS'), (u'of', 'IN'), (u'Congress', 'NNP'), (u',', ','), (u'members', 'NNS'), (u'of', 'IN'), (u'the', 'DT'), (u'Supreme', 'NNP'), (u'Court', 'NNP'), (u'and', 'CC'), (u'diplomatic', 'JJ'), (u'corps', 'NN'), (u',', ','), (u'distinguished', 'JJ'), (u'guests', 'NNS'), (u',', ','), (u'and', 'CC'), (u'fellow', 'JJ'), (u'citizens', 'NNS'), (u':', ':'), (u'Today', 'VB'), (u'our', 'PRP$'), (u'nation', 'NN'), (u'lost', 'VBD'), (u'a', 'DT'), (u'beloved', 'VBN'), (u',', ','), (u'graceful', 'JJ'), (u',', ','), (u'courageous', 'JJ'), (u'woman', 'NN'), (u'who', 'WP'), (u'called', 'VBD'), (u'America', 'NNP'), (u'to', 'TO'), (u'its', 'PRP$'), (u'founding', 'NN'), (u'ideals', 'NNS'), (u'and', 'CC'), (u'carried', 'VBD'), (u'on', 'IN'), (u'a', 'DT'), (u'noble', 'JJ'), (u'dream', 'NN'), (u'.', '.')]\n",
      "[(u'Tonight', 'NN'), (u'we', 'PRP'), (u'are', 'VBP'), (u'comforted', 'VBN'), (u'by', 'IN'), (u'the', 'DT'), (u'hope', 'NN'), (u'of', 'IN'), (u'a', 'DT'), (u'glad', 'JJ'), (u'reunion', 'NN'), (u'with', 'IN'), (u'the', 'DT'), (u'husband', 'NN'), (u'who', 'WP'), (u'was', 'VBD'), (u'taken', 'VBN'), (u'so', 'RB'), (u'long', 'RB'), (u'ago', 'RB'), (u',', ','), (u'and', 'CC'), (u'we', 'PRP'), (u'are', 'VBP'), (u'grateful', 'JJ'), (u'for', 'IN'), (u'the', 'DT'), (u'good', 'JJ'), (u'life', 'NN'), (u'of', 'IN'), (u'Coretta', 'NNP'), (u'Scott', 'NNP'), (u'King', 'NNP'), (u'.', '.')]\n",
      "[(u'(', '('), (u'Applause', 'NNP'), (u'.', '.'), (u')', ')')]\n",
      "[(u'President', 'NNP'), (u'George', 'NNP'), (u'W.', 'NNP'), (u'Bush', 'NNP'), (u'reacts', 'VBZ'), (u'to', 'TO'), (u'applause', 'VB'), (u'during', 'IN'), (u'his', 'PRP$'), (u'State', 'NNP'), (u'of', 'IN'), (u'the', 'DT'), (u'Union', 'NNP'), (u'Address', 'NNP'), (u'at', 'IN'), (u'the', 'DT'), (u'Capitol', 'NNP'), (u',', ','), (u'Tuesday', 'NNP'), (u',', ','), (u'Jan.', 'NNP'), (u'31', 'CD'), (u',', ','), (u'2006', 'CD'), (u'.', '.')]\n",
      "[(u'White', 'NNP'), (u'House', 'NNP'), (u'photo', 'NN'), (u'by', 'IN'), (u'Eric', 'NNP'), (u'DraperEvery', 'NNP'), (u'time', 'NN'), (u'I', 'PRP'), (u\"'m\", 'VBP'), (u'invited', 'JJ'), (u'to', 'TO'), (u'this', 'DT'), (u'rostrum', 'NN'), (u',', ','), (u'I', 'PRP'), (u\"'m\", 'VBP'), (u'humbled', 'VBN'), (u'by', 'IN'), (u'the', 'DT'), (u'privilege', 'NN'), (u',', ','), (u'and', 'CC'), (u'mindful', 'NN'), (u'of', 'IN'), (u'the', 'DT'), (u'history', 'NN'), (u'we', 'PRP'), (u\"'ve\", 'VBP'), (u'seen', 'VBN'), (u'together', 'RB'), (u'.', '.')]\n",
      "[(u'We', 'PRP'), (u'have', 'VBP'), (u'gathered', 'VBN'), (u'under', 'IN'), (u'this', 'DT'), (u'Capitol', 'NNP'), (u'dome', 'NN'), (u'in', 'IN'), (u'moments', 'NNS'), (u'of', 'IN'), (u'national', 'JJ'), (u'mourning', 'NN'), (u'and', 'CC'), (u'national', 'JJ'), (u'achievement', 'NN'), (u'.', '.')]\n",
      "[(u'We', 'PRP'), (u'have', 'VBP'), (u'served', 'VBN'), (u'America', 'NNP'), (u'through', 'IN'), (u'one', 'CD'), (u'of', 'IN'), (u'the', 'DT'), (u'most', 'RBS'), (u'consequential', 'JJ'), (u'periods', 'NNS'), (u'of', 'IN'), (u'our', 'PRP$'), (u'history', 'NN'), (u'--', ':'), (u'and', 'CC'), (u'it', 'PRP'), (u'has', 'VBZ'), (u'been', 'VBN'), (u'my', 'PRP$'), (u'honor', 'NN'), (u'to', 'TO'), (u'serve', 'VB'), (u'with', 'IN'), (u'you', 'PRP'), (u'.', '.')]\n",
      "[(u'In', 'IN'), (u'a', 'DT'), (u'system', 'NN'), (u'of', 'IN'), (u'two', 'CD'), (u'parties', 'NNS'), (u',', ','), (u'two', 'CD'), (u'chambers', 'NNS'), (u',', ','), (u'and', 'CC'), (u'two', 'CD'), (u'elected', 'JJ'), (u'branches', 'NNS'), (u',', ','), (u'there', 'EX'), (u'will', 'MD'), (u'always', 'RB'), (u'be', 'VB'), (u'differences', 'NNS'), (u'and', 'CC'), (u'debate', 'NN'), (u'.', '.')]\n",
      "[(u'But', 'CC'), (u'even', 'RB'), (u'tough', 'JJ'), (u'debates', 'NNS'), (u'can', 'MD'), (u'be', 'VB'), (u'conducted', 'VBN'), (u'in', 'IN'), (u'a', 'DT'), (u'civil', 'JJ'), (u'tone', 'NN'), (u',', ','), (u'and', 'CC'), (u'our', 'PRP$'), (u'differences', 'NNS'), (u'can', 'MD'), (u'not', 'RB'), (u'be', 'VB'), (u'allowed', 'VBN'), (u'to', 'TO'), (u'harden', 'VB'), (u'into', 'IN'), (u'anger', 'NN'), (u'.', '.')]\n"
     ]
    }
   ],
   "source": [
    "processContent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
